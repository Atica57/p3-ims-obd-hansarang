{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./yolov3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from models.yolo import Model\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None, image_size=512):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # boxes (x, y, w, h)\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "        # boxes (x_min, x_max, y_min, y_max)\n",
    "        boxes[:, 2] = (boxes[:, 0] + boxes[:, 2]) \n",
    "        boxes[:, 3] = (boxes[:, 1] + boxes[:, 3])\n",
    "        boxes[:, 0] /= int(self.image_size)\n",
    "        boxes[:, 1] /= int(self.image_size)\n",
    "        boxes[:, 2] /= int(self.image_size)\n",
    "        boxes[:, 3] /= int(self.image_size)\n",
    "        \n",
    "        labels = list([x['category_id'] for x in anns])\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "        \n",
    "        segmentation = np.array([x['segmentation'] for x in anns], dtype=object)\n",
    "\n",
    "        \n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = [list(box) for box in sample['bboxes']]\n",
    "            \n",
    "\n",
    "        result = []\n",
    "        labels_out = torch.zeros((len(labels), 6))\n",
    "        for i in range(len(labels)):\n",
    "            result.append(\n",
    "                [labels[i], target['boxes'][i][0], target['boxes'][i][1], target['boxes'][i][2], target['boxes'][i][3]])\n",
    "        result = np.array(result)\n",
    "\n",
    "        if len(labels):\n",
    "            labels_out[:, 1:] = torch.from_numpy(result)\n",
    "\n",
    "        return image, labels_out, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform(image_size):\n",
    "    return A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img, label, image_id = zip(*batch)  # transposed\n",
    "    for i, l in enumerate(label):\n",
    "        l[:, 0] = i  # add target image index for build_targets()\n",
    "    return torch.stack(img, 0), torch.cat(label, 0), image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(val_data_loader, model, device, conf_thres=0.001, iou_thres=0.01):\n",
    "    outputs = []\n",
    "    for images, targets, image_ids in tqdm(val_data_loader):\n",
    "        # gpu 계산을 위해 image.to(device)\n",
    "        output, _ = model(images.to(device))\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = images.shape  # batch size, channels, height, width\n",
    "        # Run NMS\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)]  # for autolabelling\n",
    "        output = non_max_suppression(output, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb)\n",
    "        for out in output:\n",
    "            outputs.append({'boxes': out[:, 0:4].data.cpu().numpy(),\n",
    "                            'scores': out[:, 4].data.cpu().numpy(), \n",
    "                            'labels': out[:, 5].data.cpu().numpy()})\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_dir = '../input/data'\n",
    "    annotation = '../input/data/train.json'\n",
    "    image_size = 512\n",
    "    val_dataset = CustomDataset(annotation, data_dir, get_valid_transform(image_size))\n",
    "    score_threshold = 0.1\n",
    "    check_point = './yolov3/weights/epoch_1.pth'\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    num_classes = 11  # 1 class (wheat) + background\n",
    "\n",
    "    model = Model(cfg='./yolov3/models/yolov3.yaml', ch=3, nc=num_classes)  # create\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    model.to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    with open('./yolov3/data/hyp.scratch.yaml') as f:\n",
    "        hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "    model.hyp = hyp\n",
    "    \n",
    "    model.load_state_dict(torch.load(check_point))\n",
    "    model.eval()\n",
    "    outputs = valid_fn(val_data_loader, model, device)\n",
    "    \n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "    coco = COCO(annotation)\n",
    "    for i, output in enumerate(outputs):\n",
    "        prediction_string = ''\n",
    "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
    "            prediction_string += str(int(label)) + ' ' + str(score) + ' ' + str(box[0]) + ' ' + str(\n",
    "                box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' '\n",
    "        prediction_strings.append(prediction_string)\n",
    "        file_names.append(image_info['file_name'])\n",
    "        \n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(f'submission.csv', index=None)\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.84s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [01:03<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.25s)\n",
      "creating index...\n",
      "index created!\n",
      "                                    PredictionString              image_id\n",
      "0  1 1.0 94.899994 76.549995 358.5 290.25 8 1.0 1...  batch_01_vt/0003.jpg\n",
      "1  6 1.0 -169.75 48.449997 313.95 345.15002 2 0.0...  batch_01_vt/0005.jpg\n",
      "2  6 1.0 -73.600006 -27.84999 339.0 390.05 4 1.0 ...  batch_01_vt/0006.jpg\n",
      "3  6 1.0 68.69998 -21.25 510.9 165.85 2 0.0021363...  batch_01_vt/0007.jpg\n",
      "4  1 1.0 -102.15 -46.199997 234.35 325.40002 2 0....  batch_01_vt/0010.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
