{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"297.278px"},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"39_DeepLabV3_Effb5_dec_dropout005_bugfixed.ipynb","provenance":[{"file_id":"1id_EH-0fPwpxuXgsl0qkforDL380jZf-","timestamp":1620225309847},{"file_id":"1l-ai3X2J6zI078rd2Qti_WtukhpPVsLI","timestamp":1620204496768},{"file_id":"1I4cN4vJ10kQK-P92qTa6lti4T8jCumHt","timestamp":1620204288557},{"file_id":"1HFnyOWzZhG57NweC2LWeOg_uKd80zvox","timestamp":1620151947229},{"file_id":"1e0PumUf9G2lLGG0YhEV4ygFgy2MASlpz","timestamp":1620117811585},{"file_id":"1DUsuvFNkPOazXYMzFTGPidr8IYQO4eow","timestamp":1620110647406},{"file_id":"12bRgXweX3GGaOcJiIZj8KbIzU6HB7anR","timestamp":1620074420951},{"file_id":"1plMVRHtqJ_PY2yaMpgMmtdakkhT3WGBt","timestamp":1620052104792},{"file_id":"18M9bYlmSv4W9Pa7wJlY3hyVhTE8PXPkr","timestamp":1619792947594},{"file_id":"1sCpy66rnkAR83jGrsjvzQ_q7sXlo0WXW","timestamp":1619783752792}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uaYHpGRsws_u","executionInfo":{"status":"ok","timestamp":1620225446845,"user_tz":-540,"elapsed":808,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}},"outputId":"d51d50ed-950b-44ff-a31f-388fa4f948f8"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Wed May  5 14:37:27 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6G-s_7kzTc3","executionInfo":{"status":"ok","timestamp":1620225462136,"user_tz":-540,"elapsed":16094,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}},"outputId":"185021ea-d91b-4795-fb93-c9d4e641fe3c"},"source":["from google.colab import drive\n","from google.colab import output\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JplG5b8Y28el","executionInfo":{"status":"ok","timestamp":1620225466521,"user_tz":-540,"elapsed":9994,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}}},"source":["!cp /content/drive/MyDrive/ml/input.zip /content"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMwnd9bP470l","executionInfo":{"status":"ok","timestamp":1620225481352,"user_tz":-540,"elapsed":24443,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}}},"source":["!unzip \"input.zip\" -d \"./input/\"\n","output.clear()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vcTYVQuBzy-c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620225482457,"user_tz":-540,"elapsed":25247,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}},"outputId":"5052540d-1b25-4581-bcf7-c2c959f8f5a5"},"source":["%cd /content/drive/MyDrive/ml/code"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/ml/code\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"toc":true,"id":"ZCuBZzzjws_p"},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#하이퍼파라미터-세팅-및-seed-고정\" data-toc-modified-id=\"하이퍼파라미터-세팅-및-seed-고정-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>하이퍼파라미터 세팅 및 seed 고정</a></span></li><li><span><a href=\"#학습-데이터-EDA\" data-toc-modified-id=\"학습-데이터-EDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>학습 데이터 EDA</a></span></li><li><span><a href=\"#데이터-전처리-함수-정의-(Dataset)\" data-toc-modified-id=\"데이터-전처리-함수-정의-(Dataset)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>데이터 전처리 함수 정의 (Dataset)</a></span></li><li><span><a href=\"#Dataset-정의-및-DataLoader-할당\" data-toc-modified-id=\"Dataset-정의-및-DataLoader-할당-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset 정의 및 DataLoader 할당</a></span><ul class=\"toc-item\"><li><span><a href=\"#데이터-샘플-시각화-(Show-example-image-and-mask)\" data-toc-modified-id=\"데이터-샘플-시각화-(Show-example-image-and-mask)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>데이터 샘플 시각화 (Show example image and mask)</a></span></li></ul></li><li><span><a href=\"#baseline-model\" data-toc-modified-id=\"baseline-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>baseline model</a></span><ul class=\"toc-item\"><li><span><a href=\"#FCN8s-(VGG-imageNet-weight)\" data-toc-modified-id=\"FCN8s-(VGG-imageNet-weight)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>FCN8s (VGG imageNet weight)</a></span></li></ul></li><li><span><a href=\"#train,-validation,-test-함수-정의\" data-toc-modified-id=\"train,-validation,-test-함수-정의-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>train, validation, test 함수 정의</a></span></li><li><span><a href=\"#모델-저장-함수-정의\" data-toc-modified-id=\"모델-저장-함수-정의-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>모델 저장 함수 정의</a></span></li><li><span><a href=\"#모델-생성-및-Loss-function,-Optimizer-정의\" data-toc-modified-id=\"모델-생성-및-Loss-function,-Optimizer-정의-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>모델 생성 및 Loss function, Optimizer 정의</a></span></li><li><span><a href=\"#저장된-model-불러오기-(학습된-이후)\" data-toc-modified-id=\"저장된-model-불러오기-(학습된-이후)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>저장된 model 불러오기 (학습된 이후)</a></span></li><li><span><a href=\"#submission을-위한-test-함수-정의\" data-toc-modified-id=\"submission을-위한-test-함수-정의-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>submission을 위한 test 함수 정의</a></span></li><li><span><a href=\"#submission.csv-생성\" data-toc-modified-id=\"submission.csv-생성-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>submission.csv 생성</a></span></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"]},{"cell_type":"markdown","metadata":{"id":"jf8EigvPws_t"},"source":["## requirements/CV2 import를 위한 lib 설치/gpu 및 메모리 상태 확인"]},{"cell_type":"code","metadata":{"tags":[],"id":"FEotYC6_ws_u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"776f35e6-7772-428c-dfca-98b26ef6a981"},"source":["from google.colab import drive\n","from google.colab import output\n","!apt-get install libsm6 libxext6 libxrender-dev\n","!pip install torch==1.4.0\n","!pip install torchvision==0.5.0\n","!pip install albumentations==0.5.2\n","!pip install segmentation_models_pytorch\n","!pip install wandb\n","!pip install madgrad\n","!pip install efficientnet_pytorch\n","\n","output.clear()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libsm6 is already the newest version (2:1.2.2-1).\n","libxext6 is already the newest version (2:1.3.3-1).\n","libxrender-dev is already the newest version (1:0.9.10-1).\n","libxrender-dev set to manually installed.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n","Collecting torch==1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:06:58.944902Z","start_time":"2021-04-22T11:06:56.623974Z"},"id":"g_So5Tcpws_v"},"source":["import os\n","import random\n","import time\n","import json\n","import warnings \n","warnings.filterwarnings('ignore')\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from utils import *\n","import cv2\n","\n","import numpy as np\n","import pandas as pd\n","\n","# 전처리를 위한 라이브러리\n","from pycocotools.coco import COCO\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# 시각화를 위한 라이브러리\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","\n","plt.rcParams['axes.grid'] = False\n","\n","print(f'pytorch version: {torch.__version__}')\n","print(f'GPU 사용 가능 여부: {torch.cuda.is_available()}')\n","\n","print(torch.cuda.get_device_name(0))\n","print(torch.cuda.device_count())\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AkrWWh1pws_v"},"source":["## 하이퍼파라미터 세팅 및 seed 고정"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:06:59.171980Z","start_time":"2021-04-22T11:06:59.167952Z"},"id":"4QhtvOP3ws_v"},"source":["train_batch_size = 4\n","valid_batch_size = 4\n","test_batch_size = 3    # test img nums = 837, have to divieded with no remainder.\n","num_epochs = 20\n","learning_rate = 1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:06:59.446510Z","start_time":"2021-04-22T11:06:59.443508Z"},"id":"sJ1qUnOMws_w"},"source":["# seed 고정\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(random_seed)\n","random.seed(random_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziQPk5jDws_w"},"source":["## 데이터 전처리 함수 정의 (Dataset)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:07:04.439837Z","start_time":"2021-04-22T11:07:04.425804Z"},"id":"LNL_eQihws_w"},"source":["category_names = ['Backgroud', 'UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing']\n","dataset_path = '/content/input/data'\n","anns_file_path = dataset_path + '/' + 'train.json'\n","\n","def get_classname(classID, cats):\n","    for i in range(len(cats)):\n","        if cats[i]['id']==classID:\n","            return cats[i]['name']\n","    return \"None\"\n","\n","class CustomDataLoader(Dataset):\n","    \"\"\"COCO format\"\"\"\n","    def __init__(self, data_dir, mode = 'train', transform = None):\n","        super().__init__()\n","        self.mode = mode\n","        self.transform = transform\n","        self.coco = COCO(data_dir)\n","        \n","    def __getitem__(self, index: int):\n","        # dataset이 index되어 list처럼 동작\n","        image_id = self.coco.getImgIds(imgIds=index)\n","        image_infos = self.coco.loadImgs(image_id)[0]\n","        \n","        # cv2 를 활용하여 image 불러오기\n","        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n","        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        images /= 255.0\n","        \n","        if (self.mode in ('train', 'val')):\n","            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n","            anns = self.coco.loadAnns(ann_ids)\n","\n","            # Load the categories in a variable\n","            cat_ids = self.coco.getCatIds()\n","            cats = self.coco.loadCats(cat_ids)\n","\n","            # masks : size가 (height x width)인 2D\n","            # 각각의 pixel 값에는 \"category id + 1\" 할당\n","            # Background = 0\n","            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n","            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n","            for i in range(len(anns)):\n","                className = get_classname(anns[i]['category_id'], cats)\n","                pixel_value = category_names.index(className)\n","                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n","            masks = masks.astype(np.float32)\n","\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images, mask=masks)\n","                images = transformed[\"image\"]\n","                masks = transformed[\"mask\"]\n","            \n","            return images, masks, image_infos\n","        \n","        if self.mode == 'test':\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images)\n","                images = transformed[\"image\"]\n","            \n","            return images, image_infos\n","    \n","    \n","    def __len__(self) -> int:\n","        # 전체 dataset의 size를 return\n","        return len(self.coco.getImgIds())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLhGnXFxws_x"},"source":["## Dataset 정의 및 DataLoader 할당"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:07:09.179806Z","start_time":"2021-04-22T11:07:04.440804Z"},"scrolled":true,"id":"4O3vbqtows_x"},"source":["# train.json / validation.json / test.json 디렉토리 설정\n","train_path = dataset_path + '/train.json'\n","val_path = dataset_path + '/val.json'\n","test_path = dataset_path + '/test.json'\n","\n","# collate_fn needs for batch\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_transform = A.Compose([\n","                            ToTensorV2()\n","                            ])\n","\n","val_transform = A.Compose([\n","                          ToTensorV2()\n","                          ])\n","\n","test_transform = A.Compose([\n","                           ToTensorV2()\n","                           ])\n","\n","# create own Dataset 1 (skip)\n","# validation set을 직접 나누고 싶은 경우\n","# random_split 사용하여 data set을 8:2 로 분할\n","# train_size = int(0.8*len(dataset))\n","# val_size = int(len(dataset)-train_size)\n","# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n","# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","# create own Dataset 2\n","# train dataset\n","train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n","\n","# validation dataset\n","val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n","\n","# test dataset\n","test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n","\n","\n","# DataLoader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=train_batch_size,\n","                                           shuffle=True,\n","                                           num_workers=0,\n","                                           collate_fn=collate_fn,\n","                                           drop_last=True)\n","\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n","                                         batch_size=valid_batch_size,\n","                                         shuffle=False,\n","                                         num_workers=0,\n","                                         collate_fn=collate_fn,\n","                                         drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=test_batch_size,\n","                                          num_workers=0,\n","                                          collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OkVLtCxws_x"},"source":["## DeeplabV3 + timm lib pretrained backbone\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:15:34.624277Z","start_time":"2021-04-22T11:15:30.068347Z"},"id":"PWyxDT2Rws_y"},"source":["from efficientnet_pytorch import EfficientNet\n","import torch.nn.functional as F\n","\n","class ASPPConv(nn.Module):\n","    def __init__(self, inplanes, outplanes, kernel_size, padding, dilation):\n","        super(ASPPConv, self).__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, outplanes, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=False)\n","        self.bn = nn.BatchNorm2d(outplanes)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(p=0.05)\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return self.drop(x)\n","\n","class ASPPPooling(nn.Module):\n","    def __init__(self, inplanes, outplanes):\n","        super(ASPPPooling, self).__init__()\n","        self.globalavgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.conv = nn.Conv2d(inplanes, outplanes, 1, stride=1, bias=False)\n","        self.bn = nn.BatchNorm2d(outplanes)\n","        self.relu = nn.ReLU()\n","        self.drop = nn.Dropout(p=0.05)\n","\n","    def forward(self, x):\n","        x = self.globalavgpool(x)\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return self.drop(x)\n","\n","\n","class ASPP(nn.Module):\n","    def __init__(self, inplanes, outplanes):\n","        super(ASPP, self).__init__()\n","        dilations = [1, 6, 12, 18]\n","        self.aspp1 = ASPPConv(inplanes, outplanes, 1, padding=0, dilation=dilations[0])\n","        self.aspp2 = ASPPConv(inplanes, outplanes, 3, padding=dilations[1], dilation=dilations[1])\n","        self.aspp3 = ASPPConv(inplanes, outplanes, 3, padding=dilations[2], dilation=dilations[2])\n","        self.aspp4 = ASPPConv(inplanes, outplanes, 3, padding=dilations[3], dilation=dilations[3])\n","        self.global_avg_pool = ASPPPooling(inplanes, outplanes)\n","        self.project = nn.Sequential(\n","            nn.Conv2d(outplanes*5, outplanes, 1, bias=False), \n","            nn.BatchNorm2d(outplanes), \n","            nn.ReLU(), \n","            nn.Dropout(p=0.05)      \n","        )\n","\n","    def forward(self, x):\n","        x1 = self.aspp1(x)\n","        x2 = self.aspp2(x)\n","        x3 = self.aspp3(x)\n","        x4 = self.aspp4(x)\n","        x5 = self.global_avg_pool(x)\n","        x5 = F.interpolate(x5, size=x.size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n","\n","        output = self.project(x)\n","        return output\n","    \n","class DeepLabHead(nn.Sequential):\n","    def __init__(self, in_ch, out_ch, n_classes):\n","        super(DeepLabHead, self).__init__()\n","        self.add_module(\"0\", ASPP(in_ch, out_ch))\n","        self.add_module(\"1\", nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1 , bias=False))\n","        self.add_module(\"2\", nn.BatchNorm2d(out_ch))\n","        self.add_module(\"3\", nn.ReLU())\n","        self.add_module(\"4\", nn.Dropout(p=0.05))\n","        self.add_module(\"5\", nn.Conv2d(out_ch, n_classes, kernel_size=1, stride=1))\n","        self.add_module(\"6\", nn.Dropout(p=0.05))\n","        \n","import timm\n","class DeepLabV3(nn.Sequential):\n","    def __init__(self, n_classes):\n","        super(DeepLabV3, self).__init__()\n","        self.backbone = EffNet()\n","        self.classifier = DeepLabHead(in_ch=512, out_ch=256, n_classes=12)\n","\n","    def forward(self, x): \n","        h = self.backbone(x)\n","        h = self.classifier(h)\n","        output = F.interpolate(h, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n","        return output\n","    \n","class EffNet(nn.Module):\n","    def __init__(self):\n","        super(EffNet, self).__init__()\n","        effnet = EfficientNet.from_pretrained('efficientnet-b5')\n","        head = nn.Sequential(effnet._conv_stem, effnet._bn0)\n","        blocks = list(effnet._blocks.children())\n","        tail = nn.Sequential(effnet._conv_head, effnet._bn1)\n","        blocks.insert(0, head)\n","        blocks.append(tail)\n","        blocks.append(nn.Conv2d(2560, 512, 1, bias=False))  # projection\n","        self.backbone = nn.Sequential(*blocks)\n","\n","    def forward(self, x):\n","        output = self.backbone(x)\n","        return output\n","\n","\n","# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n","model = DeepLabV3(n_classes=12)\n","\n","x = torch.randn([1, 3, 512, 512])\n","print(\"input shape : \", x.shape)\n","model.eval()\n","out = model(x).to(device)\n","print(\"output shape : \", out.size())\n","\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcHcNzh_ws_y"},"source":["## wandb 설정 및 train, validation, test 함수 정의"]},{"cell_type":"code","metadata":{"id":"LnCv-I8Ews_y"},"source":["import wandb\n","\n","# Start a new run\n","wandb.init(project='kwangwon', entity='pstage12')\n","\n","# Save model inputs and hyperparameters\n","config = wandb.config\n","config.learning_rate = learning_rate\n","config.train_batch_size = train_batch_size\n","config.num_epochs = num_epochs\n","\n","# Log gradients and model parameters\n","wandb.watch(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:15:38.201874Z","start_time":"2021-04-22T11:15:38.187884Z"},"id":"oDwy3_fPws_y"},"source":["from tqdm import tqdm\n","from utils import dice_loss\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","def train(num_epochs, model, data_loader, val_loader, criterion1, criterion2, optimizer, saved_dir, val_every, device):\n","    print('Start training..')\n","    best_loss = 9999999\n","    best_mIoU = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        for step, (images, masks, _) in tqdm(enumerate(data_loader)):\n","            \n","            images = torch.stack(images)       # (batch, channel, height, width)\n","            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n","            \n","            # gpu 연산을 위해 device 할당\n","            images, masks = images.to(device), masks.to(device)\n","                  \n","            # inference\n","            outputs = model(images)\n","            \n","            #pr = outputs.detach().cpu().numpy()\n","            #gt = masks.detach().cpu().numpy()\n","            #print(pr.shape)\n","            #print(gt.shape)\n","            \n","            # loss 계산\n","            loss = criterion1(outputs, masks) + criterion2(outputs, masks)\n","            total_loss += loss.item()\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            # LR schedule 적용\n","            lr_scheduler.step()\n","            \n","            current_lr = get_lr(optimizer)\n","            loss_train_avg = total_loss / (step+1)\n","            # step 주기에 따른 loss 출력\n","            if (step + 1) % 25 == 0:\n","                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_loader)}], Loss: {loss_train_avg:.4f}, LR: {current_lr}')\n","                wandb.log({\"Train loss\": loss_train_avg})\n","            \n","            #mIoU at specific step\n","            #if (step + 1) % 100 == 0:\n","            #    outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n","            #    mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n","            #    print(f'mIoU: {mIoU:.4f}')\n","            #    wandb.log({\"Train mIoU\": mIoU})\n","        \n","        # validation 주기에 따른 loss 출력 및 best model 저장\n","        if (epoch + 1) % val_every == 0:\n","            avrg_loss, avrg_mIoU = validation(epoch + 1, model, val_loader, criterion1, criterion2, device)\n","            if avrg_loss < best_loss: # 둘 중 하나라도 best라면, 저장.\n","                print(f'mininum loss at epoch: {epoch + 1}')\n","                print('Save model in', saved_dir)\n","                best_loss = avrg_loss\n","                save_model(model, saved_dir)\n","                \n","            if avrg_mIoU > best_mIoU:\n","                print(f'max mIoU at epoch: {epoch + 1}')\n","                print('Save model in', saved_dir)\n","                best_mIoU = avrg_mIoU\n","                save_model(model, saved_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:15:38.901226Z","start_time":"2021-04-22T11:15:38.888195Z"},"id":"-KLFWBU0ws_z"},"source":["from utils import _fast_hist\n","def validation(epoch, model, data_loader, criterion1, criterion2, device):\n","    print(f'Start validation #{epoch}')\n","    model.eval()\n","    with torch.no_grad():\n","        total_loss = 0\n","        cnt = 0\n","        n_class = 12\n","        hist = np.zeros((n_class, n_class))\n","        for step, (images, masks, _) in enumerate(data_loader):\n","            \n","            images = torch.stack(images)       # (batch, channel, height, width)\n","            masks = torch.stack(masks).long()  # (batch, height, width)\n","\n","            images, masks = images.to(device), masks.to(device)            \n","\n","            outputs = model(images)\n","            loss = criterion1(outputs, masks) + criterion2(outputs, masks)\n","            total_loss += loss\n","            cnt += 1\n","            \n","            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n","\n","            # 각각의 mask에 대한 confusion matrix를 hist에 저장\n","            for lt, lp in zip(outputs, masks.detach().cpu().numpy()):\n","                hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n","            \n","        avrg_loss = total_loss / cnt\n","        avrg_mIoU = label_accuracy_score(hist)\n","        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, avrg_mIoU))\n","        wandb.log({\"Valid Avg loss\": avrg_loss})\n","        wandb.log({\"Valid Avg mIoU\": avrg_mIoU})\n","\n","    return avrg_loss, avrg_mIoU"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4C8gAgltws_z"},"source":["## 모델 저장 함수 정의"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:15:41.634492Z","start_time":"2021-04-22T11:15:41.627493Z"},"id":"PpKabMVows_z"},"source":["# 모델 저장 함수 정의\n","val_every = 1 \n","\n","saved_dir = './saved'\n","if not os.path.isdir(saved_dir):                                                           \n","    os.mkdir(saved_dir)\n","    \n","def save_model(model, saved_dir, file_name='DeepLabV3_Effb5_dec_dropout005_bugfixed.pt'):\n","    check_point = {'net': model.state_dict()}\n","    output_path = os.path.join(saved_dir, file_name)\n","    torch.save(model.state_dict(), output_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7JCXX7k8ws_z"},"source":["## 모델 생성 및 Loss function, Optimizer 정의"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-22T11:15:43.106368Z","start_time":"2021-04-22T11:15:43.096368Z"},"id":"U7ViggUQws_z"},"source":["from madgrad import MADGRAD\n","\n","# Loss function 정의\n","criterion1 = FocalLoss()\n","criterion2 = nn.CrossEntropyLoss()\n","\n","# Optimizer 정의\n","#Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)\n","optimizer = MADGRAD(params = model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 0.0001, eps = 1e-06)\n","\n","# Lr_scheculer 정의\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 654, gamma = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2021-04-22T11:15:43.700Z"},"id":"0KdnKt-7ws_z"},"source":["train(num_epochs, model, train_loader, val_loader, criterion1, criterion2, optimizer, saved_dir, val_every, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LazqX9lZws_0"},"source":["## 저장된 model 불러오기 (학습된 이후) "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-16T19:44:21.050200Z","start_time":"2021-04-16T19:44:20.802200Z"},"scrolled":true,"id":"vOg_UW6Gws_0"},"source":["# best model 저장된 경로\n","model_path = './saved/DeepLabV3_Effb5_dec_dropout005_bugfixed.pt'\n","\n","# best model 불러오기\n","checkpoint = torch.load(model_path, map_location=device)\n","model.load_state_dict(checkpoint)\n","\n","# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n","# model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-16T19:44:24.939227Z","start_time":"2021-04-16T19:44:24.518228Z"},"id":"ct4HPgylws_0"},"source":["# 첫번째 batch의 추론 결과 확인\n","for imgs, image_infos in test_loader:\n","    image_infos = image_infos\n","    temp_images = imgs\n","    \n","    model.eval()\n","    # inference\n","    outs = model(torch.stack(temp_images).to(device))\n","    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n","    \n","    break\n","\n","i = 2\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n","\n","print('Shape of Original Image :', list(temp_images[i].shape))\n","print('Shape of Predicted : ', list(oms[i].shape))\n","print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n","\n","# Original image\n","ax1.imshow(temp_images[i].permute([1,2,0]))\n","ax1.grid(False)\n","ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n","\n","# Predicted\n","ax2.imshow(oms[i])\n","ax2.grid(False)\n","ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AwT4E0xjws_0"},"source":["## submission을 위한 test 함수 정의"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-16T19:44:27.469285Z","start_time":"2021-04-16T19:44:27.456021Z"},"id":"ku0VHEa3ws_0"},"source":["def test(model, data_loader, device):\n","    size = 256\n","    transform = A.Compose([A.Resize(256, 256)])\n","    print('Start prediction.')\n","    model.eval()\n","    \n","    file_name_list = []\n","    preds_array = np.empty((0, size*size), dtype=np.long)\n","    \n","    with torch.no_grad():\n","        for step, (imgs, image_infos) in tqdm(enumerate(test_loader)):\n","\n","            # inference (512 x 512)\n","            outs = model(torch.stack(imgs).to(device))\n","            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n","            \n","            # resize (256 x 256)\n","            temp_mask = []\n","            for img, mask in zip(np.stack(temp_images), oms):\n","                transformed = transform(image=img, mask=mask)\n","                mask = transformed['mask']\n","                temp_mask.append(mask)\n","\n","            oms = np.array(temp_mask)\n","            \n","            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n","            preds_array = np.vstack((preds_array, oms))\n","            \n","            file_name_list.append([i['file_name'] for i in image_infos])\n","    print(\"End prediction.\")\n","    file_names = [y for x in file_name_list for y in x]\n","    \n","    return file_names, preds_array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QztYU6a7ws_0"},"source":["## submission.csv 생성"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-16T19:45:42.235310Z","start_time":"2021-04-16T19:44:30.499016Z"},"scrolled":true,"id":"AbNeaE98ws_0","outputId":"ed84faa9-c9e3-4599-c752-3a145cb00054"},"source":["# sample_submisson.csv 열기\n","submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n","\n","# test set에 대한 prediction\n","file_names, preds = test(model, test_loader, device)\n","\n","# PredictionString 대입\n","for file_name, string in zip(file_names, preds):\n","    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n","                                   ignore_index=True)\n","\n","# submission.csv로 저장\n","submission.to_csv(\"./submission/DeepLabV3_Effb5_dec_dropout005_bugfixed.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Start prediction.\n"],"name":"stdout"},{"output_type":"stream","text":["93it [18:51, 12.17s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["End prediction.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pn-WRBofws_1"},"source":["criterion1(outputs, masks) + criterion2(outputs, masks)## Reference\n","\n"]}]}