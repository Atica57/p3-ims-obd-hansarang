{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from pycocotools.coco import COCO\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "cat_histogram = np.zeros(nr_cats, dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns=[\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "\n",
    "class NormalizedAugmentation:\n",
    "    '''\n",
    "    기본값인 mean 과 std를 사용한 Augmentation\n",
    "    '''\n",
    "    def __init__(self, mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25), **args):\n",
    "        self.transform = A.Compose([\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image)\n",
    "\n",
    "class NewAugmentation:\n",
    "    '''\n",
    "    기본값인 mean 과 std를 사용한 Augmentation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25), **args):\n",
    "        self.transform = A.Compose([\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        self.train_transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.Rotate(p=0.3, limit=45),\n",
    "            A.Cutout(num_holes=4, max_h_size=20, max_w_size=20),\n",
    "            A.CLAHE(),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "#A.Cutout(num_holes=4, max_h_size=20, max_w_size=20),\n",
    "\n",
    "\n",
    "    def __call__(self, image,mode):\n",
    "        if mode=='train':\n",
    "            return self.train_transform(image)\n",
    "        elif mode=='val':\n",
    "            return self.transform(image)\n",
    "        elif mode=='test':\n",
    "            return self.transform(image)\n",
    "\n",
    "class PseudoTrainset(data.Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir,mode = 'train'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = None\n",
    "        self.coco = COCO(data_dir)\n",
    "        self.dataset_path = '../input/data/'\n",
    "        self.category_names = ['Backgroud', 'UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing']\n",
    "        \n",
    "        self.pseudo_imgs = np.load(self.dataset_path+'pseudo_imgs_path.npy')\n",
    "        self.pseudo_masks = sorted(glob.glob(self.dataset_path+'pseudo_masks/*.npy'))\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        ### Train data ###\n",
    "        if (index < len(self.coco.getImgIds())):\n",
    "            image_id = self.coco.getImgIds(imgIds=index)\n",
    "            image_infos = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "            images = cv2.imread(self.dataset_path+image_infos['file_name'])\n",
    "            images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "            images /= 255.0\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "            \n",
    "            ###  mask 생성  ###\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = self.category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "\n",
    "        ### Pseudo data ###\n",
    "        else:\n",
    "            index -= len(self.coco.getImgIds())\n",
    "            images = cv2.imread(self.dataset_path+self.pseudo_imgs[index])\n",
    "            images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "            masks = np.load(self.pseudo_masks[index])\n",
    "            \n",
    "        ###  augmentation ###\n",
    "        masks = masks.astype(np.float32)\n",
    "        if self.transform is not None:\n",
    "                if self.mode=='train':\n",
    "                    transformed = self.transform.train_transform(image=images, mask=masks)\n",
    "                    images = transformed[\"image\"]\n",
    "                    masks = transformed[\"mask\"]\n",
    "                elif self.mode=='val':\n",
    "                    transformed = self.transform.transform(image=images, mask=masks)\n",
    "                    images = transformed[\"image\"]\n",
    "                    masks = transformed[\"mask\"]\n",
    "                elif self.mode=='test':\n",
    "                    transformed = self.transform.transform(image=images)\n",
    "                    images = transformed[\"image\"]\n",
    "            \n",
    "        return images, masks\n",
    "    \n",
    "    def set_transform(self, transform):\n",
    "        '''\n",
    "        :param transform: 우리가 원하는 transform으로 dataset의 transform을 설정해준다\n",
    "        '''\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coco.getImgIds())+len(self.pseudo_imgs)\n",
    "\n",
    "class TrashDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, mode='train'):\n",
    "        self.transform = None\n",
    "        self.mode = mode\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(self.data_dir)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            annss = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(annss)):\n",
    "                className = self.get_classname(annss[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(annss[i]) * pixel_value, masks)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                if self.mode=='train':\n",
    "                    transformed = self.transform.train_transform(image=images, mask=masks)\n",
    "                    images = transformed[\"image\"]\n",
    "                    masks = transformed[\"mask\"]\n",
    "                elif self.mode=='val':\n",
    "                    transformed = self.transform.transform(image=images, mask=masks)\n",
    "                    images = transformed[\"image\"]\n",
    "                    masks = transformed[\"mask\"]\n",
    "\n",
    "            return images, masks, image_infos\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "\n",
    "            return images, image_infos\n",
    "\n",
    "    def get_classname(self, classID, cats):\n",
    "        for i in range(len(cats)):\n",
    "            if cats[i]['id'] == classID:\n",
    "                return cats[i]['name']\n",
    "        return \"None\"\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        '''\n",
    "        :param transform: 우리가 원하는 transform으로 dataset의 transform을 설정해준다\n",
    "        '''\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(n_class * label_true[mask].astype(int) + label_pred[mask],\n",
    "                        minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(hist):\n",
    "    \"\"\"\n",
    "    Returns accuracy score evaluation result.\n",
    "      - [acc]: overall accuracy\n",
    "      - [acc_cls]: mean accuracy\n",
    "      - [mean_iu]: mean IU\n",
    "      - [fwavacc]: fwavacc\n",
    "    \"\"\"\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc\n",
    "\n",
    "\n",
    "def add_hist(hist, label_trues, label_preds, n_class):\n",
    "    \"\"\"\n",
    "        stack hist(confusion matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "class CosineAnnealingWarmUpRestart(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestart, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (\n",
    "                        1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma ** self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36ng24t8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 94248<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from adamp import AdamP\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def increment_path(path, exist_ok=False):\n",
    "    \"\"\" Automatically increment path, i.e. runs/exp --> runs/exp0, runs/exp1 etc.\n",
    "\n",
    "    Args:\n",
    "        path (str or pathlib.Path): f\"{model_dir}/{args.name}\".\n",
    "        exist_ok (bool): whether increment path (increment if False).\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\"\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def train(data_dir, model_dir):\n",
    "    train_path = data_dir + '/train_all.json'\n",
    "    val_path = data_dir + '/val.json'\n",
    "\n",
    "    seed_everything(42)\n",
    "    save_dir = './'+increment_path(os.path.join(model_dir, 'oneloss+alldata'))\n",
    "    os.makedirs(save_dir)\n",
    "    # -- settings\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"PyTorch version:[%s].\" % (torch.__version__))\n",
    "    GPU_NUM = 0 # 원하는 GPU 번호 입력\n",
    "    device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"device:[%s].\" % (device))\n",
    "\n",
    "    # -- dataset  # default: BaseAugmentation\n",
    "    train_dataset = PseudoTrainset(\n",
    "        data_dir=train_path,\n",
    "        mode='train'\n",
    "    )\n",
    "    val_dataset = TrashDataset(\n",
    "        data_dir=val_path,\n",
    "        mode='val'\n",
    "    )\n",
    "\n",
    "    # -- augmentation  # default: BaseAugmentation\n",
    "    transform = NewAugmentation()\n",
    "    train_dataset.set_transform(transform)\n",
    "    val_dataset.set_transform(transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=4,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # -- model  # default: BaseModel\n",
    "    #model_path = './model/exp/best.pth'\n",
    "    #checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = smp.DeepLabV3Plus(\n",
    "        encoder_name=\"timm-efficientnet-b5\",\n",
    "        encoder_weights=\"noisy-student\",\n",
    "        in_channels=3,\n",
    "        classes=12,\n",
    "    ).to(device)\n",
    "    #model.load_state_dict(checkpoint)\n",
    "    \n",
    "    wandb.watch(model)\n",
    "    criterion1 = FocalLoss(gamma=0.5)\n",
    "    #criterion2 = nn.CrossEntropyLoss(weight=normed_weights)\n",
    "    #criterion2 = smp.losses.DiceLoss(mode='multiclass')\n",
    "    criterion2 = smp.losses.SoftCrossEntropyLoss(smooth_factor=0.1)\n",
    "    optimizer = AdamP(\n",
    "        model.parameters(),\n",
    "        lr=5e-6,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-4,\n",
    "        eps=1e-6\n",
    "    )\n",
    "    # optimizer = AdamP(model.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=1e-2)\n",
    "    # optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
    "    #scheduler = StepLR(optimizer, 1, gamma=0.7)\n",
    "    scheduler = CosineAnnealingWarmUpRestart(optimizer, T_0=4, T_mult=1, eta_max=2e-4,  T_up=1, gamma=0.75)\n",
    "    # -- logging\n",
    "        \n",
    "        \n",
    "        \n",
    "    best_mIoU = 0\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(1,21):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        for idx, (images, masks) in enumerate(tqdm(train_loader)):\n",
    "            images = torch.stack(images)  # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            #loss1 = criterion1(outputs, masks)\n",
    "            loss = criterion2(outputs, masks)\n",
    "            #loss = 0.75*loss1+0.25*loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            if (idx + 1) % 20 == 0:\n",
    "                train_loss = loss_value / 20\n",
    "                current_lr = get_lr(optimizer)\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{20}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || lr {current_lr}\"\n",
    "                )\n",
    "                wandb.log({\"train loss\": train_loss})\n",
    "                loss_value = 0\n",
    "        hist = np.zeros((12, 12))\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            for idx, (images, masks, _) in enumerate(val_loader):\n",
    "                images = torch.stack(images)  # (batch, channel, height, width)\n",
    "                masks = torch.stack(masks).long()\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                #loss1 = criterion1(outputs, masks)\n",
    "                loss = criterion2(outputs, masks)\n",
    "                #loss = 0.75*loss1+0.25*loss2\n",
    "                val_loss_items.append(loss)\n",
    "                outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "                hist = add_hist(hist, masks.detach().cpu().numpy(), outputs, n_class=12)\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            best_val_loss = min(best_val_loss, val_loss)\n",
    "            _, _,mIoU,_ = label_accuracy_score(hist)\n",
    "            wandb.log({\n",
    "                        \"Test mIoU\": mIoU,\n",
    "                        \"Test Loss\": val_loss})\n",
    "            if mIoU > best_mIoU:\n",
    "                print(f\"New best model for val accuracy : {mIoU:4.2%}! saving the best model..\")\n",
    "                torch.save(model.state_dict(), f\"{save_dir}/best.pth\")\n",
    "                best_mIoU = mIoU\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/last.pth\")\n",
    "            print(\n",
    "                f\"[Val] mIoU : {mIoU:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best mIoU : {best_mIoU:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )\n",
    "        scheduler.step()\n",
    "        # val loop\n",
    "\n",
    "\n",
    "import wandb\n",
    "data_dir = os.environ.get('SM_CHANNEL_TRAIN', '../input/data')\n",
    "model_dir = os.environ.get('SM_MODEL_DIR', './model')\n",
    "wandb.init(project='chowon', entity='pstage12')\n",
    "wandb.run.name = 'oneloss+alldata'\n",
    "train(data_dir, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.4.0].\n",
      "device:[cuda].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Start prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/93 [00:09<13:54,  9.07s/it]\u001b[A\n",
      "  2%|▏         | 2/93 [00:17<13:15,  8.75s/it]\u001b[A\n",
      "  3%|▎         | 3/93 [00:25<12:51,  8.57s/it]\u001b[A\n",
      "  4%|▍         | 4/93 [00:33<12:28,  8.42s/it]\u001b[A\n",
      "  5%|▌         | 5/93 [00:41<12:13,  8.34s/it]\u001b[A\n",
      "  6%|▋         | 6/93 [00:49<11:56,  8.24s/it]\u001b[A\n",
      "  8%|▊         | 7/93 [00:57<11:48,  8.24s/it]\u001b[A\n",
      "  9%|▊         | 8/93 [01:05<11:40,  8.24s/it]\u001b[A\n",
      " 10%|▉         | 9/93 [01:15<12:03,  8.61s/it]\u001b[A\n",
      " 11%|█         | 10/93 [01:23<11:40,  8.44s/it]\u001b[A\n",
      " 12%|█▏        | 11/93 [01:31<11:21,  8.31s/it]\u001b[A\n",
      " 13%|█▎        | 12/93 [01:39<11:10,  8.28s/it]\u001b[A\n",
      " 14%|█▍        | 13/93 [01:47<11:00,  8.25s/it]\u001b[A\n",
      " 15%|█▌        | 14/93 [01:56<10:51,  8.24s/it]\u001b[A\n",
      " 16%|█▌        | 15/93 [02:04<10:45,  8.28s/it]\u001b[A\n",
      " 17%|█▋        | 16/93 [02:12<10:37,  8.27s/it]\u001b[A\n",
      " 18%|█▊        | 17/93 [02:20<10:29,  8.28s/it]\u001b[A\n",
      " 19%|█▉        | 18/93 [02:29<10:27,  8.37s/it]\u001b[A\n",
      " 20%|██        | 19/93 [02:38<10:22,  8.42s/it]\u001b[A\n",
      " 22%|██▏       | 20/93 [02:46<10:15,  8.43s/it]\u001b[A\n",
      " 23%|██▎       | 21/93 [02:55<10:09,  8.46s/it]\u001b[A\n",
      " 24%|██▎       | 22/93 [03:03<09:58,  8.42s/it]\u001b[A\n",
      " 25%|██▍       | 23/93 [03:11<09:51,  8.45s/it]\u001b[A\n",
      " 26%|██▌       | 24/93 [03:20<09:44,  8.47s/it]\u001b[A\n",
      " 27%|██▋       | 25/93 [03:28<09:33,  8.44s/it]\u001b[A\n",
      " 28%|██▊       | 26/93 [03:37<09:26,  8.46s/it]\u001b[A\n",
      " 29%|██▉       | 27/93 [03:45<09:17,  8.45s/it]\u001b[A\n",
      " 30%|███       | 28/93 [03:54<09:09,  8.46s/it]\u001b[A\n",
      " 31%|███       | 29/93 [04:02<09:03,  8.50s/it]\u001b[A\n",
      " 32%|███▏      | 30/93 [04:11<08:55,  8.50s/it]\u001b[A\n",
      " 33%|███▎      | 31/93 [04:19<08:42,  8.43s/it]\u001b[A\n",
      " 34%|███▍      | 32/93 [04:28<08:36,  8.46s/it]\u001b[A\n",
      " 35%|███▌      | 33/93 [04:36<08:24,  8.41s/it]\u001b[A\n",
      " 37%|███▋      | 34/93 [04:45<08:23,  8.53s/it]\u001b[A\n",
      " 38%|███▊      | 35/93 [04:53<08:15,  8.55s/it]\u001b[A\n",
      " 39%|███▊      | 36/93 [05:02<08:11,  8.63s/it]\u001b[A\n",
      " 40%|███▉      | 37/93 [05:11<08:01,  8.59s/it]\u001b[A\n",
      " 41%|████      | 38/93 [05:19<07:51,  8.58s/it]\u001b[A\n",
      " 42%|████▏     | 39/93 [05:28<07:39,  8.51s/it]\u001b[A\n",
      " 43%|████▎     | 40/93 [05:36<07:27,  8.44s/it]\u001b[A\n",
      " 44%|████▍     | 41/93 [05:44<07:16,  8.40s/it]\u001b[A\n",
      " 45%|████▌     | 42/93 [05:52<07:07,  8.38s/it]\u001b[A\n",
      " 46%|████▌     | 43/93 [06:01<06:57,  8.34s/it]\u001b[A\n",
      " 47%|████▋     | 44/93 [06:09<06:47,  8.31s/it]\u001b[A\n",
      " 48%|████▊     | 45/93 [06:17<06:40,  8.33s/it]\u001b[A\n",
      " 49%|████▉     | 46/93 [06:26<06:32,  8.35s/it]\u001b[A\n",
      " 51%|█████     | 47/93 [06:34<06:24,  8.37s/it]\u001b[A\n",
      " 52%|█████▏    | 48/93 [06:43<06:17,  8.39s/it]\u001b[A\n",
      " 53%|█████▎    | 49/93 [06:51<06:08,  8.38s/it]\u001b[A\n",
      " 54%|█████▍    | 50/93 [06:59<06:02,  8.42s/it]\u001b[A\n",
      " 55%|█████▍    | 51/93 [07:08<05:56,  8.48s/it]\u001b[A\n",
      " 56%|█████▌    | 52/93 [07:17<05:47,  8.48s/it]\u001b[A\n",
      " 57%|█████▋    | 53/93 [07:25<05:38,  8.47s/it]\u001b[A\n",
      " 58%|█████▊    | 54/93 [07:33<05:28,  8.42s/it]\u001b[A\n",
      " 59%|█████▉    | 55/93 [07:42<05:26,  8.61s/it]\u001b[A\n",
      " 60%|██████    | 56/93 [07:51<05:22,  8.72s/it]\u001b[A\n",
      " 61%|██████▏   | 57/93 [08:00<05:10,  8.63s/it]\u001b[A\n",
      " 62%|██████▏   | 58/93 [08:08<04:59,  8.56s/it]\u001b[A\n",
      " 63%|██████▎   | 59/93 [08:16<04:48,  8.50s/it]\u001b[A\n",
      " 65%|██████▍   | 60/93 [08:25<04:39,  8.48s/it]\u001b[A\n",
      " 66%|██████▌   | 61/93 [08:33<04:29,  8.42s/it]\u001b[A\n",
      " 67%|██████▋   | 62/93 [08:42<04:20,  8.39s/it]\u001b[A\n",
      " 68%|██████▊   | 63/93 [08:51<04:19,  8.67s/it]\u001b[A\n",
      " 69%|██████▉   | 64/93 [08:59<04:09,  8.59s/it]\u001b[A\n",
      " 70%|██████▉   | 65/93 [09:08<03:58,  8.52s/it]\u001b[A\n",
      " 71%|███████   | 66/93 [09:16<03:49,  8.50s/it]\u001b[A\n",
      " 72%|███████▏  | 67/93 [09:24<03:39,  8.43s/it]\u001b[A\n",
      " 73%|███████▎  | 68/93 [09:33<03:29,  8.40s/it]\u001b[A\n",
      " 74%|███████▍  | 69/93 [09:41<03:21,  8.38s/it]\u001b[A\n",
      " 75%|███████▌  | 70/93 [09:49<03:13,  8.40s/it]\u001b[A\n",
      " 76%|███████▋  | 71/93 [09:58<03:04,  8.39s/it]\u001b[A\n",
      " 77%|███████▋  | 72/93 [10:06<02:56,  8.40s/it]\u001b[A\n",
      " 78%|███████▊  | 73/93 [10:15<02:48,  8.41s/it]\u001b[A\n",
      " 80%|███████▉  | 74/93 [10:23<02:40,  8.45s/it]\u001b[A\n",
      " 81%|████████  | 75/93 [10:32<02:32,  8.45s/it]\u001b[A\n",
      " 82%|████████▏ | 76/93 [10:40<02:23,  8.42s/it]\u001b[A\n",
      " 83%|████████▎ | 77/93 [10:48<02:14,  8.41s/it]\u001b[A\n",
      " 84%|████████▍ | 78/93 [10:57<02:06,  8.40s/it]\u001b[A\n",
      " 85%|████████▍ | 79/93 [11:05<01:57,  8.41s/it]\u001b[A\n",
      " 86%|████████▌ | 80/93 [11:14<01:49,  8.44s/it]\u001b[A\n",
      " 87%|████████▋ | 81/93 [11:23<01:43,  8.65s/it]\u001b[A\n",
      " 88%|████████▊ | 82/93 [11:31<01:34,  8.59s/it]\u001b[A\n",
      " 89%|████████▉ | 83/93 [11:40<01:25,  8.54s/it]\u001b[A\n",
      " 90%|█████████ | 84/93 [11:48<01:16,  8.49s/it]\u001b[A\n",
      " 91%|█████████▏| 85/93 [11:57<01:08,  8.52s/it]\u001b[A\n",
      " 92%|█████████▏| 86/93 [12:05<00:59,  8.55s/it]\u001b[A\n",
      " 94%|█████████▎| 87/93 [12:14<00:51,  8.55s/it]\u001b[A\n",
      " 95%|█████████▍| 88/93 [12:22<00:42,  8.55s/it]\u001b[A\n",
      " 96%|█████████▌| 89/93 [12:31<00:34,  8.53s/it]\u001b[A\n",
      " 97%|█████████▋| 90/93 [12:39<00:25,  8.52s/it]\u001b[A\n",
      " 98%|█████████▊| 91/93 [12:48<00:17,  8.52s/it]\u001b[A\n",
      " 99%|█████████▉| 92/93 [12:57<00:08,  8.58s/it]\u001b[A\n",
      "100%|██████████| 93/93 [13:06<00:00,  8.45s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import pandas as pd \n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"PyTorch version:[%s].\" % (torch.__version__))\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"device:[%s].\" % (device))\n",
    "    \n",
    "model_path = './model/Augmentation_bestparam_nocontrast/best.pth'\n",
    "dataset_path = '../input/data'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# best model 불러오기\n",
    "batch_size = 9\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"timm-efficientnet-b5\",\n",
    "    encoder_weights=\"noisy-student\",\n",
    "    in_channels=3,\n",
    "    classes=12\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "test_dataset = TrashDataset(data_dir=test_path, mode='test')  # default: NormalizedAugmentation\n",
    "transform = NormalizedAugmentation()\n",
    "test_dataset.set_transform(transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transformer = A.Compose([A.Resize(256, 256)])\n",
    "    tta = A.HorizontalFlip(p=1)\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size * size), dtype=np.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(tqdm(data_loader)):\n",
    "            \n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transformer(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "\n",
    "            oms = oms.reshape([oms.shape[0], size * size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "\n",
    "    return file_names, preds_array\n",
    "\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append(\n",
    "        {\"image_id\": file_name, \"PredictionString\": ' '.join(str(e) for e in string.tolist())},\n",
    "        ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(\"./submission/hahahoho.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
